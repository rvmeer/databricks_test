{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e3c746e-83ab-412e-9b51-be3a84ad8a19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Configuration, and read in of all the available CSV files from the raw data storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f7b95b-8f84-4b4a-af2c-09ffc56845b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import to_timestamp, col, year, month, when\n",
    "\n",
    "container = \"databricks\"\n",
    "storage_account = \"experimentcidt\"\n",
    "tenant_id = \"80a5cb6b-ae21-4ea8-bd3f-25e005cefc5b\"\n",
    "managed_identity_client_id = \"33f7c0cd-4fa6-432d-9be6-225da9c1768b\"\n",
    "\n",
    "# Get the storage account key from the secret scope, which is connected to Azure data vault\n",
    "storage_account_key = dbutils.secrets.get(scope=\"Experiments3\", key=\"storage-account-key\")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\",\n",
    "    f\"{storage_account_key}\"\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "df_with_source = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .csv(f\"wasbs://{container}@{storage_account}.blob.core.windows.net/*.csv\") \\\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "# Alternatively, create a dataframe from all CSV files in: /Volumes/experiments3/raw_logging/vol1/csvs/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7bee7ef-7431-4d2a-860d-1c82328f6bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Some initial data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365f098f-495d-40c3-bc30-330969ef806d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove all rows where the user is 'NULL'\n",
    "df_with_source = df_with_source.filter(df_with_source.user != \"NULL\")\n",
    "\n",
    "# Remove all rows where cpd_name is 'NULL'\n",
    "df_with_source = df_with_source.filter(df_with_source.cpd_name != \"NULL\")\n",
    "\n",
    "# Remove all rows where CPD_NAME is in ['DIVEIN', 'TSTCPD', 'CCBB']\n",
    "df_with_source = df_with_source.filter(~df_with_source.cpd_name.isin([\"DIVEIN\", \"TSTCPD\", \"CCBB\"]))\n",
    "\n",
    "# Remove all rows where user is in ['rameer', 'kwijk', 'rwithage', 'simoncox']\n",
    "df_with_source = df_with_source.filter(~df_with_source.user.isin([\"rameer\", \"kwijk\", \"rwithage\", \"simoncox\"]))\n",
    "\n",
    "# Remove all rows where logger is 'linuxFileSystemWatcher'\n",
    "# There is some serious error here that clutters the logging\n",
    "df_with_source = df_with_source.filter(df_with_source.logger != \"linuxFileSystemWatcher\") \n",
    "\n",
    "# Remove duplicate rows where _time and user and cpd_name are the same\n",
    "# Because we have multiple CSV they overlap.\n",
    "df_with_source = df_with_source.dropDuplicates([\"_time\", \"user\", \"cpd_name\"])\n",
    "\n",
    "# Remove when the message contains 'Checker found an error in DDF'. The DDF checker pollutes\n",
    "# the messages. This message has to become DEBUG level.\n",
    "df_with_source = df_with_source.filter(~df_with_source.message.contains(\"Checker found an error in DDF\"))\n",
    "\n",
    "# Print the total number of rows\n",
    "print(f\"Total number of rows: {df_with_source.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc7f88d-2878-48ef-95eb-3f6ae6618c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For later partitioning, we need a year and month column. Also add a iso_timestamp column having an ISO formatted timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f544cd-c4f4-4e06-baeb-48029bbe57dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The _time column looks like 'Sun Jan 19 14:20:55 2025', create a iso_timestamp column \n",
    "df_with_source = df_with_source.withColumn(\"iso_timestamp\", to_timestamp(col(\"_time\"), \"EEE MMM dd HH:mm:ss yyyy\"))\n",
    "\n",
    "# Add a year and month column, for later partitioning of the delta table\n",
    "df_with_source = df_with_source.withColumn(\"year\", year(col(\"iso_timestamp\")))\n",
    "df_with_source = df_with_source.withColumn(\"month\", month(col(\"iso_timestamp\")))\n",
    "\n",
    "# Print the min and max timestamp in the dataframe\n",
    "print(f\"Min timestamp: {df_with_source.agg({'iso_timestamp': 'min'}).collect()[0][0]}\")\n",
    "print(f\"Max timestamp: {df_with_source.agg({'iso_timestamp': 'max'}).collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f3c1d6-10af-44f6-b05c-30c655d6043e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a feature column, having the feature name obtained from the logger and/or the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbfc97c-c1ae-4be5-95e8-b812f75f6644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Features\n",
    "#####\n",
    "\n",
    "# If the logger column contains 'devbenchExtension' and the message is 'Debugger attached successfully',\n",
    "# put 'Python remote debugging' in a new 'feature' column. Else put 'NULL' into this column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"devbenchExtension\") & col(\"message\").contains(\"Debugger attached successfully\"), \"Python remote debugging\").otherwise(\"NULL\"))\n",
    "\n",
    "# If the logger column contains 'reportPreviewExtension' put 'Report preview' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"reportPreviewExtension\"), \"Report preview\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contain 'ddfDefinitionProvider' put 'DDF jump around' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"ddfDefinitionProvider\"), \"DDF jump around\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'aspectsExtension' put 'CPD aspects overview' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"aspectsExtension\"), \"CPD aspects overview\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'ddfFileTreeExtension' put 'Required interfaces viewer' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"ddfFileTreeExtension\"), \"Required interfaces viewer\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'devbenchExtension' and the message contains 'Adding filewatcher' put 'Devbench sync, automatic file upload' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"devbenchExtension\") & col(\"message\").contains(\"Adding filewatcher\"), \"Devbench sync, automatic file upload\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'devbenchExtension' and the message contains 'successfully created' put 'Devbench integration' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"devbenchExtension\") & col(\"message\").contains(\"successfully created\"), \"Devbench integration\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'reportEditor' put 'Report editor' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"reportEditor\"), \"Report editor\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contain 'ddfCheckerExtension' put 'DDF syntax checking' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"ddfCheckerExtension\"), \"DDF syntax checking\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'flowEditor' put 'Flow editor' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"flowEditor\"), \"Flow editor\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'devbenchExtension' and the message contains 'on ER/ER_event_log' put 'ER eventlog viewer' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"devbenchExtension\") & col(\"message\").contains(\"on ER/ER_event_log\"), \"ER eventlog viewer\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'devbenchExtension' and the message contains 'Live sync enabled for' put 'Live sync' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"devbenchExtension\") & col(\"message\").contains(\"Live sync enabled for\"), \"Live sync\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'swipeExtension' put 'Swipe integration' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"swipeExtension\"), \"Swipe integration\").otherwise(col(\"feature\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e039493-9e47-4afc-847b-3a5031a305d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Determine which CPDs are classic CPDs. Put True/False in the classic column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009a387b-d202-490b-94d0-b1a711bfa6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classic_cpd_names = spark.table(\"experiments3.logging.classic_cpd_names\")\n",
    "classic_cpd_names.show(200, False)\n",
    "\n",
    "# For every row in df_with_source, if the cpd_name column contains a value from classic_cpd_names, put True in the classic column, else False\n",
    "df_with_source = df_with_source.withColumn(\"classic\", when(col(\"cpd_name\").isin(classic_cpd_names.select(\"cpd_name\").rdd.flatMap(lambda x: x).collect()), True).otherwise(False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9c22f9-eea1-4c44-849b-3fcdca52f6b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write out the combined table, partition by year and month\n",
    "df_with_source.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").saveAsTable(\"experiments3.logging.combined\")\n",
    "\n",
    "# Print the nummer of rows written\n",
    "print(f\"Total number of rows written: {df_with_source.count()} to table: experiments3.logging.combined\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingest CSVs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
