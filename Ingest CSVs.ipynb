{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f7b95b-8f84-4b4a-af2c-09ffc56845b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import to_timestamp, col, year, month, when\n",
    "\n",
    "# sp=r&st=2025-01-20T14:06:31Z&se=2026-01-01T22:06:31Z&spr=https&sv=2022-11-02&sr=c&sig=Dg8hCI6bX1FpOJ1Jb5suhsR12ZPs1Pm%2Bw484GkfMNU4%3D\n",
    "# https://experimentcidt.blob.core.windows.net/databricks?sp=r&st=2025-01-20T14:06:31Z&se=2026-01-01T22:06:31Z&spr=https&sv=2022-11-02&sr=c&sig=Dg8hCI6bX1FpOJ1Jb5suhsR12ZPs1Pm%2Bw484GkfMNU4%3D\n",
    "\n",
    "container = \"databricks\"\n",
    "storage_account = \"experimentcidt\"\n",
    "tenant_id = \"80a5cb6b-ae21-4ea8-bd3f-25e005cefc5b\"\n",
    "managed_identity_client_id = \"33f7c0cd-4fa6-432d-9be6-225da9c1768b\"\n",
    "\n",
    "storage_account_key = dbutils.secrets.get(scope=\"Experiments3\", key=\"storage-account-key\")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\",\n",
    "    f\"{storage_account_key}\"\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "\n",
    "\n",
    "df_with_source = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .csv(f\"wasbs://{container}@{storage_account}.blob.core.windows.net/*.csv\") \\\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "# Remove all rows where the user is 'NULL'\n",
    "df_with_source = df_with_source.filter(df_with_source.user != \"NULL\")\n",
    "\n",
    "# Remove all rows where cpd_name is 'NULL'\n",
    "df_with_source = df_with_source.filter(df_with_source.cpd_name != \"NULL\")\n",
    "\n",
    "# Remove all rows where logger is 'linuxFileSystemWatcher'\n",
    "df_with_source = df_with_source.filter(df_with_source.logger != \"linuxFileSystemWatcher\") \n",
    "\n",
    "# Remove duplicate rows where _time and user and cpd_name are the same\n",
    "df_with_source = df_with_source.dropDuplicates([\"_time\", \"user\", \"cpd_name\"])\n",
    "\n",
    "# Print the total number of rows\n",
    "print(f\"Total number of rows: {df_with_source.count()}\")\n",
    "\n",
    "# The _time column looks like 'Sun Jan 19 14:20:55 2025', create a iso_timestamp column \n",
    "df_with_source = df_with_source.withColumn(\"iso_timestamp\", to_timestamp(col(\"_time\"), \"EEE MMM dd HH:mm:ss yyyy\"))\n",
    "\n",
    "# Add a year and month column, for later partitioning of the delta table\n",
    "df_with_source = df_with_source.withColumn(\"year\", year(col(\"iso_timestamp\")))\n",
    "df_with_source = df_with_source.withColumn(\"month\", month(col(\"iso_timestamp\")))\n",
    "\n",
    "#####\n",
    "# Features\n",
    "#####\n",
    "\n",
    "# If the logger column contains 'devbenchExtension' and the message is 'Debugger attached successfully',\n",
    "# put 'Python remote debugging' in a new 'feature' column. Else put 'NULL' into this column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"devbenchExtension\") & col(\"message\").contains(\"Debugger attached successfully\"), \"Python remote debugging\").otherwise(\"NULL\"))\n",
    "\n",
    "# If the logger column contains 'reportPreviewExtension' put 'Report preview' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"reportPreviewExtension\"), \"Report preview\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contain 'ddfDefinitionProvider' put 'DDF jump around' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"ddfDefinitionProvider\"), \"DDF jump around\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'aspectsExtension' put 'CPD aspects overview' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"aspectsExtension\"), \"CPD aspects overview\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'ddfFileTreeExtension' put 'Required interfaces viewer' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"ddfFileTreeExtension\"), \"Required interfaces viewer\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'devbenchExtension' and the message contains 'Adding filewatcher' put 'Devbench sync, automatic file upload' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"devbenchExtension\") & col(\"message\").contains(\"Adding filewatcher\"), \"Devbench sync, automatic file upload\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'devbenchExtension' and the message contains 'successfully created' put 'Devbench integration' in the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"devbenchExtension\") & col(\"message\").contains(\"successfully created\"), \"Devbench integration\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'reportEditor' put 'Report editor' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"reportEditor\"), \"Report editor\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contain 'ddfCheckerExtension' put 'DDF syntax checking' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"ddfCheckerExtension\"), \"DDF syntax checking\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'flowEditor' put 'Flow editor' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"flowEditor\"), \"Flow editor\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'devbenchExtension' and the message contains 'on ER/ER_event_log' put 'ER eventlog viewer' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"devbenchExtension\") & col(\"message\").contains(\"on ER/ER_event_log\"), \"ER eventlog viewer\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'devbenchExtension' and the message contains 'Live sync enabled for' put 'Live sync' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"devbenchExtension\") & col(\"message\").contains(\"Live sync enabled for\"), \"Live sync\").otherwise(col(\"feature\")))\n",
    "\n",
    "# If the logger column contains 'swipeExtension' put 'Swipe integration' to the feature column\n",
    "df_with_source = df_with_source.withColumn(\"feature\", when(col(\"logger\").contains(\"swipeExtension\"), \"Swipe integration\").otherwise(col(\"feature\")))\n",
    "\n",
    "#####\n",
    "\n",
    "# Now only print the time, user and cpd_name columns, do not restrict the column display length\n",
    "df_with_source.select(\"iso_timestamp\", \"user\", \"cpd_name\").show(10, False)\n",
    "\n",
    "# Write out the combined table, partition by year and month\n",
    "df_with_source.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").saveAsTable(\"experiments3.logging.combined\")\n",
    "\n",
    "# Print the nummer of rows written\n",
    "print(f\"Total number of rows written: {df_with_source.count()}\")\n",
    "\n",
    "#df_with_source.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"experiments3.logging.combined\")\n",
    "\n",
    "#df.write \\\n",
    "#    .format(\"delta\") \\\n",
    "#    .partitionBy(\"jaar\", \"maand\") \\\n",
    "#    .mode(\"append\") \\\n",
    "#    .save(\"/mnt/delta/combined_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingest CSVs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
